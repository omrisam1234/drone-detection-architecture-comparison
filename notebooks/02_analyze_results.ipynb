{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Drone Detection - Results Analysis\n\nEvaluate trained models on the test set and generate comparison plots."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Configuration"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport tempfile\nfrom pathlib import Path\n\nimport cv2\nimport yaml\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom ultralytics import YOLO, RTDETR\n\n# Paths\nPROJECT_ROOT = Path('..').resolve()\nARTIFACTS_DIR = PROJECT_ROOT / 'artifacts'\nDATA_DIR = PROJECT_ROOT / 'data' / 'dataset'\nRESULTS_DIR = PROJECT_ROOT / 'results'\n\nRESULTS_DIR.mkdir(parents=True, exist_ok=True)\n(RESULTS_DIR / 'error_analysis').mkdir(exist_ok=True)\n\n# Error analysis settings\nN_ERROR_EXAMPLES = 5\nIOU_THRESHOLD = 0.5\n\n# Plot styling\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.rcParams['figure.figsize'] = (12, 6)\nplt.rcParams['font.size'] = 11\nplt.rcParams['axes.titlesize'] = 14\nplt.rcParams['axes.labelsize'] = 12\n\nMODEL_COLORS = {\n    'yolo11l': '#2ecc71',\n    'rtdetr-l': '#e74c3c',\n}\n\nMODEL_LABELS = {\n    'yolo11l': 'YOLO11-L (CNN)',\n    'rtdetr-l': 'RT-DETR-L (Attention)',\n}\n\nMODEL_MARKERS = {\n    'yolo11l': 'o',\n    'rtdetr-l': 's',\n}\n\nprint(f\"Artifacts: {ARTIFACTS_DIR}\")\nprint(f\"Results:   {RESULTS_DIR}\")\nprint(f\"PyTorch:   {torch.__version__}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Discover Artifacts"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_artifacts(artifacts_dir):\n",
    "    runs = []\n",
    "    \n",
    "    for run_dir in artifacts_dir.iterdir():\n",
    "        if not run_dir.is_dir() or run_dir.name.startswith('.'):\n",
    "            continue\n",
    "        \n",
    "        parts = run_dir.name.rsplit('_', 1)\n",
    "        if len(parts) != 2:\n",
    "            continue\n",
    "        \n",
    "        model_name = parts[0]\n",
    "        try:\n",
    "            dataset_size = int(parts[1])\n",
    "        except ValueError:\n",
    "            continue\n",
    "        \n",
    "        train_dir = run_dir / 'train'\n",
    "        if not train_dir.exists():\n",
    "            continue\n",
    "        \n",
    "        results_csv = train_dir / 'results.csv'\n",
    "        \n",
    "        # Collect checkpoints\n",
    "        weights_dir = train_dir / 'weights'\n",
    "        checkpoints = []\n",
    "        if weights_dir.exists():\n",
    "            for pt_file in weights_dir.glob('*.pt'):\n",
    "                if pt_file.stem.startswith('epoch'):\n",
    "                    try:\n",
    "                        epoch = int(pt_file.stem.replace('epoch', ''))\n",
    "                        checkpoints.append({'epoch': epoch, 'path': pt_file})\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "                elif pt_file.stem == 'best':\n",
    "                    checkpoints.append({'epoch': 'best', 'path': pt_file})\n",
    "        \n",
    "        runs.append({\n",
    "            'model': model_name,\n",
    "            'dataset_size': dataset_size,\n",
    "            'run_dir': train_dir,\n",
    "            'results_csv': results_csv if results_csv.exists() else None,\n",
    "            'checkpoints': checkpoints,\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(runs)\n",
    "    if len(df) > 0:\n",
    "        df = df.sort_values(['model', 'dataset_size']).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "artifacts_df = discover_artifacts(ARTIFACTS_DIR)\n",
    "\n",
    "if len(artifacts_df) == 0:\n",
    "    raise FileNotFoundError(f\"No artifacts found in {ARTIFACTS_DIR}. Run notebook 01 first.\")\n",
    "\n",
    "print(f\"Found {len(artifacts_df)} training runs:\\n\")\n",
    "for _, row in artifacts_df.iterrows():\n",
    "    n_ckpts = len(row['checkpoints'])\n",
    "    has_csv = \"yes\" if row['results_csv'] else \"no\"\n",
    "    print(f\"  {row['model']:12} | {row['dataset_size']:5} images | {n_ckpts} checkpoints | CSV: {has_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Training Metrics"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_training_metrics(artifacts_df):\n    all_metrics = []\n    \n    for _, row in artifacts_df.iterrows():\n        if row['results_csv'] is None:\n            continue\n        df = pd.read_csv(row['results_csv'])\n        df.columns = df.columns.str.strip()  # ultralytics adds spaces\n        df['model'] = row['model']\n        df['dataset_size'] = row['dataset_size']\n        all_metrics.append(df)\n    \n    if not all_metrics:\n        return pd.DataFrame()\n    return pd.concat(all_metrics, ignore_index=True)\n\n\ntraining_metrics = load_training_metrics(artifacts_df)\n\nif len(training_metrics) == 0:\n    raise ValueError(\"No training metrics found\")\n\n# column names vary across ultralytics versions\nmap50_cols = [c for c in training_metrics.columns if 'mAP50' in c and '95' not in c]\nmap50_95_cols = [c for c in training_metrics.columns if 'mAP50-95' in c]\n\nMAP50_COL = map50_cols[0] if map50_cols else None\nMAP50_95_COL = map50_95_cols[0] if map50_95_cols else None\n\nprint(f\"Loaded {len(training_metrics)} rows\")\nprint(f\"mAP50 column:    {MAP50_COL}\")\nprint(f\"mAP50-95 column: {MAP50_95_COL}\")\n\ntraining_metrics.to_csv(RESULTS_DIR / 'training_metrics.csv', index=False)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Convergence Curves"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_convergence_curves(metrics_df, map_col, output_path):\n",
    "    dataset_sizes = sorted(metrics_df['dataset_size'].unique())\n",
    "    n_sizes = len(dataset_sizes)\n",
    "    \n",
    "    if n_sizes <= 3:\n",
    "        n_rows, n_cols = 1, n_sizes\n",
    "    else:\n",
    "        n_rows = 2\n",
    "        n_cols = (n_sizes + 1) // 2\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 5 * n_rows), sharey=True)\n",
    "    axes = np.array(axes).flatten()\n",
    "    \n",
    "    for idx, dataset_size in enumerate(dataset_sizes):\n",
    "        ax = axes[idx]\n",
    "        subset = metrics_df[metrics_df['dataset_size'] == dataset_size]\n",
    "        \n",
    "        for model in sorted(subset['model'].unique()):\n",
    "            model_data = subset[subset['model'] == model].sort_values('epoch')\n",
    "            ax.plot(\n",
    "                model_data['epoch'], model_data[map_col],\n",
    "                color=MODEL_COLORS.get(model, 'gray'),\n",
    "                marker=MODEL_MARKERS.get(model, 'o'),\n",
    "                markersize=4, linewidth=2,\n",
    "                label=MODEL_LABELS.get(model, model),\n",
    "            )\n",
    "        \n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_title(f'{dataset_size} Training Images')\n",
    "        ax.legend(loc='upper left', fontsize=9)\n",
    "        ax.set_ylim(0, 1)\n",
    "    \n",
    "    for idx in range(n_sizes, len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    for idx in range(0, len(axes), n_cols):\n",
    "        if idx < n_sizes:\n",
    "            axes[idx].set_ylabel('Validation mAP50-95')\n",
    "    \n",
    "    plt.suptitle('Convergence Curves: CNN vs Attention', fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved to {output_path}\")\n",
    "\n",
    "\n",
    "if MAP50_95_COL:\n",
    "    plot_convergence_curves(training_metrics, MAP50_95_COL, RESULTS_DIR / 'convergence_curves.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Test Set Evaluation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def evaluate_checkpoint(checkpoint_path, data_dir, model_type):\n    if 'rtdetr' in model_type.lower():\n        model = RTDETR(checkpoint_path)\n    else:\n        model = YOLO(checkpoint_path)\n    \n    # need a temp yaml because ultralytics wants a file path\n    data_yaml = {\n        'train': str(data_dir / 'train' / 'images'),\n        'val': str(data_dir / 'valid' / 'images'),\n        'test': str(data_dir / 'test' / 'images'),\n        'nc': 1,\n        'names': ['drone']\n    }\n    \n    with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f:\n        yaml.dump(data_yaml, f)\n        temp_yaml = f.name\n    \n    results = model.val(data=temp_yaml, split='test', verbose=False)\n    Path(temp_yaml).unlink()\n    \n    return {\n        'mAP50': results.results_dict.get('metrics/mAP50(B)', 0),\n        'mAP50-95': results.results_dict.get('metrics/mAP50-95(B)', 0),\n        'precision': results.results_dict.get('metrics/precision(B)', 0),\n        'recall': results.results_dict.get('metrics/recall(B)', 0),\n    }"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "best_ckpts = []\nfor _, row in artifacts_df.iterrows():\n    for ckpt in row['checkpoints']:\n        if ckpt['epoch'] == 'best':\n            best_ckpts.append({'model': row['model'], 'dataset_size': row['dataset_size'], 'path': ckpt['path']})\n\nprint(f\"Evaluating {len(best_ckpts)} checkpoints...\\n\")\n\ntest_results = []\nfor ckpt in tqdm(best_ckpts):\n    metrics = evaluate_checkpoint(ckpt['path'], DATA_DIR, ckpt['model'])\n    test_results.append({'model': ckpt['model'], 'dataset_size': ckpt['dataset_size'], 'epoch': 'best', 'checkpoint': str(ckpt['path']), **metrics})\n\ntest_results_df = pd.DataFrame(test_results)\ntest_results_df.to_csv(RESULTS_DIR / 'test_evaluation.csv', index=False)\nprint(f\"\\nEvaluated {len(test_results_df)} checkpoints\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Data Efficiency"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_efficiency(results_df, output_path):\n",
    "    final_df = results_df[results_df['epoch'] == 'best'].copy()\n",
    "    if len(final_df) == 0:\n",
    "        max_epoch = results_df['epoch'].max()\n",
    "        final_df = results_df[results_df['epoch'] == max_epoch].copy()\n",
    "    \n",
    "    dataset_sizes = sorted(final_df['dataset_size'].unique())\n",
    "    models = sorted(final_df['model'].unique())\n",
    "    \n",
    "    x = np.arange(len(dataset_sizes))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    for ax, metric, title in zip(axes, ['mAP50', 'mAP50-95'], ['mAP50', 'mAP50-95']):\n",
    "        for i, model in enumerate(models):\n",
    "            model_data = final_df[final_df['model'] == model].set_index('dataset_size')\n",
    "            values = [model_data.loc[ds, metric] if ds in model_data.index else 0 for ds in dataset_sizes]\n",
    "            \n",
    "            offset = width * (i - len(models)/2 + 0.5)\n",
    "            bars = ax.bar(\n",
    "                x + offset, values, width,\n",
    "                label=MODEL_LABELS.get(model, model),\n",
    "                color=MODEL_COLORS.get(model, 'gray'),\n",
    "            )\n",
    "            \n",
    "            for bar, val in zip(bars, values):\n",
    "                ax.annotate(f'{val:.2f}',\n",
    "                    xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                    ha='center', va='bottom', fontsize=8, rotation=45)\n",
    "        \n",
    "        ax.set_xlabel('Training Dataset Size')\n",
    "        ax.set_ylabel(title)\n",
    "        ax.set_title(f'Test Set {title}')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(dataset_sizes)\n",
    "        ax.legend()\n",
    "        ax.set_ylim(0, 1.15)\n",
    "    \n",
    "    plt.suptitle('Data Efficiency: CNN vs Attention', fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved to {output_path}\")\n",
    "\n",
    "\n",
    "if len(test_results_df) > 0:\n",
    "    plot_data_efficiency(test_results_df, RESULTS_DIR / 'data_efficiency.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Error Analysis"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def compute_iou(box1, box2):\n    x1 = max(box1[0], box2[0])\n    y1 = max(box1[1], box2[1])\n    x2 = min(box1[2], box2[2])\n    y2 = min(box1[3], box2[3])\n    \n    inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n    union_area = box1_area + box2_area - inter_area\n    \n    return inter_area / union_area if union_area > 0 else 0\n\n\ndef load_ground_truth(label_path, img_width, img_height):\n    \"\"\"YOLO format: class x_center y_center width height (normalized)\"\"\"\n    boxes = []\n    if not label_path.exists():\n        return boxes\n    \n    with open(label_path) as f:\n        for line in f:\n            parts = line.strip().split()\n            if len(parts) >= 5:\n                _, xc, yc, w, h = map(float, parts[:5])\n                # convert to pixel coords\n                x1 = (xc - w/2) * img_width\n                y1 = (yc - h/2) * img_height\n                x2 = (xc + w/2) * img_width\n                y2 = (yc + h/2) * img_height\n                boxes.append([x1, y1, x2, y2])\n    \n    return boxes\n\n\ndef check_detection_correct(pred_boxes, gt_boxes, iou_thresh=0.5):\n    \"\"\"True if every GT box has a matching prediction\"\"\"\n    if len(gt_boxes) == 0:\n        return len(pred_boxes) == 0\n    if len(pred_boxes) == 0:\n        return False\n    \n    for gt in gt_boxes:\n        matched = any(compute_iou(pred, gt) >= iou_thresh for pred in pred_boxes)\n        if not matched:\n            return False\n    return True"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def visualize_error_examples(examples, title, output_path):\n    if not examples:\n        print(f\"No examples for: {title}\")\n        return\n    \n    n = len(examples)\n    fig, axes = plt.subplots(1, n, figsize=(5 * n, 5))\n    if n == 1:\n        axes = [axes]\n    \n    for ax, ex in zip(axes, examples):\n        img = cv2.imread(str(ex['image_path']))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        ax.imshow(img)\n        \n        for box in ex['gt_boxes']:\n            rect = plt.Rectangle((box[0], box[1]), box[2]-box[0], box[3]-box[1],\n                fill=False, edgecolor='blue', linewidth=2)\n            ax.add_patch(rect)\n        \n        for i, box in enumerate(ex['yolo_boxes']):\n            conf = ex['yolo_conf'][i] if i < len(ex['yolo_conf']) else 0\n            rect = plt.Rectangle((box[0], box[1]), box[2]-box[0], box[3]-box[1],\n                fill=False, edgecolor='green', linewidth=2, linestyle='--')\n            ax.add_patch(rect)\n            ax.text(box[0], box[1]-5, f'YOLO:{conf:.2f}', color='green', fontsize=8)\n        \n        for i, box in enumerate(ex['rtdetr_boxes']):\n            conf = ex['rtdetr_conf'][i] if i < len(ex['rtdetr_conf']) else 0\n            rect = plt.Rectangle((box[0], box[1]), box[2]-box[0], box[3]-box[1],\n                fill=False, edgecolor='red', linewidth=2, linestyle=':')\n            ax.add_patch(rect)\n            ax.text(box[2], box[1]-5, f'DETR:{conf:.2f}', color='red', fontsize=8)\n        \n        ax.set_title(ex['image_path'].name, fontsize=9)\n        ax.axis('off')\n    \n    from matplotlib.patches import Patch\n    legend_elements = [\n        Patch(facecolor='none', edgecolor='blue', label='Ground Truth'),\n        Patch(facecolor='none', edgecolor='green', linestyle='--', label='YOLO'),\n        Patch(facecolor='none', edgecolor='red', linestyle=':', label='RT-DETR'),\n    ]\n    fig.legend(handles=legend_elements, loc='upper center', ncol=3, bbox_to_anchor=(0.5, 1.05))\n    \n    plt.suptitle(title, fontsize=14, y=1.1)\n    plt.tight_layout()\n    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n    plt.show()\n    print(f\"Saved to {output_path}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def run_error_analysis(yolo_ckpt, rtdetr_ckpt, data_dir, output_dir, n_examples=5, iou_thresh=0.5):\n    yolo_model = YOLO(yolo_ckpt)\n    rtdetr_model = RTDETR(rtdetr_ckpt)\n    \n    test_images_dir = data_dir / 'test' / 'images'\n    test_labels_dir = data_dir / 'test' / 'labels'\n    \n    test_images = list(test_images_dir.glob('*.jpg'))\n    \n    print(f\"Running inference on {len(test_images)} test images...\")\n    \n    yolo_wins = []    # yolo correct, rtdetr wrong\n    rtdetr_wins = []  # rtdetr correct, yolo wrong\n    both_fail = []\n    both_succeed = []\n    \n    for img_path in tqdm(test_images):\n        img = cv2.imread(str(img_path))\n        if img is None:\n            continue\n        h, w = img.shape[:2]\n        \n        label_path = test_labels_dir / (img_path.stem + '.txt')\n        gt_boxes = load_ground_truth(label_path, w, h)\n        \n        yolo_res = yolo_model.predict(img_path, verbose=False)[0]\n        rtdetr_res = rtdetr_model.predict(img_path, verbose=False)[0]\n        \n        yolo_boxes = yolo_res.boxes.xyxy.cpu().numpy().tolist() if len(yolo_res.boxes) > 0 else []\n        rtdetr_boxes = rtdetr_res.boxes.xyxy.cpu().numpy().tolist() if len(rtdetr_res.boxes) > 0 else []\n        \n        yolo_ok = check_detection_correct(yolo_boxes, gt_boxes, iou_thresh)\n        rtdetr_ok = check_detection_correct(rtdetr_boxes, gt_boxes, iou_thresh)\n        \n        result = {\n            'image_path': img_path,\n            'gt_boxes': gt_boxes,\n            'yolo_boxes': yolo_boxes,\n            'rtdetr_boxes': rtdetr_boxes,\n            'yolo_conf': yolo_res.boxes.conf.cpu().numpy().tolist() if len(yolo_res.boxes) > 0 else [],\n            'rtdetr_conf': rtdetr_res.boxes.conf.cpu().numpy().tolist() if len(rtdetr_res.boxes) > 0 else [],\n        }\n        \n        if yolo_ok and not rtdetr_ok:\n            yolo_wins.append(result)\n        elif rtdetr_ok and not yolo_ok:\n            rtdetr_wins.append(result)\n        elif not yolo_ok and not rtdetr_ok:\n            both_fail.append(result)\n        else:\n            both_succeed.append(result)\n    \n    print(f\"\\nYOLO wins: {len(yolo_wins)} | RT-DETR wins: {len(rtdetr_wins)} | Both fail: {len(both_fail)} | Both succeed: {len(both_succeed)}\")\n    \n    return {\n        'yolo_wins': yolo_wins[:n_examples],\n        'rtdetr_wins': rtdetr_wins[:n_examples],\n        'both_fail': both_fail[:n_examples],\n    }"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# compare models on largest dataset only\nprint(\"Error Analysis\\n\")\n\nmax_size = artifacts_df['dataset_size'].max()\nyolo_best = None\nrtdetr_best = None\n\nfor _, row in artifacts_df.iterrows():\n    if row['dataset_size'] != max_size:\n        continue\n    for ckpt in row['checkpoints']:\n        if ckpt['epoch'] == 'best':\n            if row['model'] == 'yolo11l':\n                yolo_best = ckpt['path']\n            elif row['model'] == 'rtdetr-l':\n                rtdetr_best = ckpt['path']\n\nif yolo_best and rtdetr_best:\n    print(f\"YOLO:    {yolo_best}\")\n    print(f\"RT-DETR: {rtdetr_best}\\n\")\n\n    error_results = run_error_analysis(\n        yolo_ckpt=yolo_best,\n        rtdetr_ckpt=rtdetr_best,\n        data_dir=DATA_DIR,\n        output_dir=RESULTS_DIR / 'error_analysis',\n        n_examples=N_ERROR_EXAMPLES,\n        iou_thresh=IOU_THRESHOLD,\n    )\nelse:\n    print(\"Missing best.pt checkpoints for error analysis\")\n    error_results = None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize error examples\n",
    "if error_results:\n",
    "    if error_results['yolo_wins']:\n",
    "        visualize_error_examples(\n",
    "            error_results['yolo_wins'],\n",
    "            'YOLO Correct, RT-DETR Wrong',\n",
    "            RESULTS_DIR / 'error_analysis' / 'yolo_wins.png'\n",
    "        )\n",
    "    \n",
    "    if error_results['rtdetr_wins']:\n",
    "        visualize_error_examples(\n",
    "            error_results['rtdetr_wins'],\n",
    "            'RT-DETR Correct, YOLO Wrong',\n",
    "            RESULTS_DIR / 'error_analysis' / 'rtdetr_wins.png'\n",
    "        )\n",
    "    \n",
    "    if error_results['both_fail']:\n",
    "        visualize_error_examples(\n",
    "            error_results['both_fail'],\n",
    "            'Both Models Failed',\n",
    "            RESULTS_DIR / 'error_analysis' / 'both_fail.png'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Output"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List generated files\n",
    "for f in sorted(RESULTS_DIR.rglob('*')):\n",
    "    if f.is_file():\n",
    "        size_kb = f.stat().st_size / 1024\n",
    "        print(f\"  {f.relative_to(RESULTS_DIR)} ({size_kb:.1f} KB)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}